mean(subsetCollegedata2$Apps)
knitr::opts_chunk$set(echo = FALSE)
#Install required packages
library(ggplot2)
library(leaps)
library(glmnet)
library("VIM")
library(corrplot)
# Import data and get general information
collegedata <- read.csv("/Users/rtf/Google Drive (fiscrt17@wfu.edu)/Fourth Year/STA363/Project 2/collegedata.csv")
ncol(collegedata)
nrow(collegedata)
#Encode private from Yes/No to 1/0
collegedata$Private <- ifelse(collegedata$Private=='Yes', 1, 0)
#Note 1: remove enroll column and University Column
subsetCollegedata = collegedata[c(-1,-5)]
#Note 2: add "Rate" column = Accept / Apps and remove accept
subsetCollegedata["Rate"] = subsetCollegedata["Accept"] / subsetCollegedata["Apps"]
subsetCollegedata2 = subsetCollegedata[c(-3)]
mean(subsetCollegedata2$Apps)
#Fit lasso model
cv.out.2 <- cv.glmnet(XD[, -1], subsetCollegedata2$Apps, alpha = 1, lambda = seq(from = 0, to = 1000, by = .5) )
knitr::opts_chunk$set(echo = FALSE)
#Install required packages
library(ggplot2)
library(leaps)
library(glmnet)
library("VIM")
library(corrplot)
# Import data and get general information
collegedata <- read.csv("/Users/rtf/Google Drive (fiscrt17@wfu.edu)/Fourth Year/STA363/Project 2/collegedata.csv")
ncol(collegedata)
nrow(collegedata)
#Encode private from Yes/No to 1/0
collegedata$Private <- ifelse(collegedata$Private=='Yes', 1, 0)
#Note 1: remove enroll column and University Column
subsetCollegedata = collegedata[c(-1,-5)]
#Note 2: add "Rate" column = Accept / Apps and remove accept
subsetCollegedata["Rate"] = subsetCollegedata["Accept"] / subsetCollegedata["Apps"]
subsetCollegedata2 = subsetCollegedata[c(-3)]
aggr_plot <- aggr(subsetCollegedata2, col = c("navyblue", 'red'), numbers=TRUE, sortVars=TRUE, labels=names(subsetCollegedata2), cex.axis=0.7, gap=3, ylab=c("Histogram of missing data", "Pattern"))
#Get information on rows and columns of cleaned dataset
ncol(subsetCollegedata2)
nrow(subsetCollegedata2)
#Run BSS
BSSfull <- regsubsets(Apps ~ ., data = subsetCollegedata2)
summary(BSSfull)
#Plot metrics of BSS
plot(BSSfull, scale = "adjr2")
#See coefficients of final model
finalModel1 = lm(Apps ~ F.Undergrad + Expend + Grad.Rate + Rate, data=subsetCollegedata2)
summary(finalModel1)
#Choose k
k <- 10
#Define n
n <- nrow(subsetCollegedata2)
#Create storage
residualsKFOLD1 <- matrix(NA, nrow = n, ncol=1)
#Create folds
set.seed(123)
folds <- sample(rep(1:k, 78), n, replace=FALSE)
for(i in 1:k){
#Find the rows in fold i
infold <- which(folds==i)
#Create training set
CVTrain <- subsetCollegedata2[-infold,]
#Create test set
CVTest <- subsetCollegedata2[infold,]
lmCV1 <- lm(Apps ~ F.Undergrad + Expend + Grad.Rate + Rate, data=CVTrain)
#Predict on CV Test data
predCV1 <- predict(lmCV1, newdata=CVTest)
#Compute the MSRE for each fold and store
residualsKFOLD1[infold] <- CVTest$Apps - predCV1
}
#Compute the average MSRE across all folds
RMSE1 = sqrt(mean((residualsKFOLD1)^2))
#Check correlation of predictors
corrplot(cor(subsetCollegedata2), method="circle")
#run ridge
XD <- model.matrix(Apps ~ ., data =subsetCollegedata2)
ridge.mod <- glmnet(XD[, -1], subsetCollegedata2$Apps, alpha = 0, standardize = TRUE)
set.seed(1)
cv.out <- cv.glmnet(XD[, -1], subsetCollegedata2$Apps, alpha=0, lambda=seq(from = 0, to = 1000, by = 0.5))
plot(cv.out, main="MSE vs Lambda Value, 10 fold CV. Figure 2")
#Compute final ridge model and show coefficients
ridge.final <- glmnet(XD[, -1], subsetCollegedata2$Apps, alpha=0, lambda= cv.out$lambda.min)
#predict(ridge.final, type = "coefficients", s = cv.out$lambda.min)
sqrt(min(cv.out$cvm))
Mat <- cbind(FullModel = coefficients(glmnet(XD[ , -1], subsetCollegedata2$Apps, alpha =0 ,lambda = 0 )), Shrinkage = coefficients(ridge.final))
Mat <-as.matrix(Mat)
colnames(Mat) <-c("Full Model", "Shrinkage")
Mat <- data.frame(Mat)
knitr::kable(Mat, caption="Table 1")
#Fit lasso model
cv.out.2 <- cv.glmnet(XD[, -1], subsetCollegedata2$Apps, alpha = 1, lambda = seq(from = 0, to = 1000, by = .5) )
#Fit lasso model
cv.out.2 <- cv.glmnet(XD[, -1], subsetCollegedata2$Apps, alpha = 1, lambda = seq(from = 0, to = 1000, by = .5) )
plot(cv.out.2)
cv.out.2$lambda.min
sqrt(cv.out.2$cvm)
sqrt(min(cv.out.2$cvm))
Mat <- cbind(FullModel = coefficients(glmnet(XD[ , -1], subsetCollegedata2$Apps, alpha =0 ,lambda = 0 )), Shrinkage = coefficients(ridge.final), Lasso = coefficients(lasso.final))
#Compute final ridge model and show coefficients
lasso.final <- glmnet(XD[, -1], subsetCollegedata2$Apps, alpha=1, lambda= cv.out.2$lambda.min)
predict(lasso.final, type = "coefficients", s = cv.out$lambda.min)
sqrt(min(cv.out.2$cvm))
Mat <- cbind(FullModel = coefficients(glmnet(XD[ , -1], subsetCollegedata2$Apps, alpha =0 ,lambda = 0 )), Shrinkage = coefficients(ridge.final), Lasso = coefficients(lasso.final))
Mat <-as.matrix(Mat)
colnames(Mat) <-c("Full Model", "Shrinkage", "Lasso")
Mat <- data.frame(Mat)
knitr::kable(Mat, caption = "Table 2")
#Install required packages
library(ggplot2)
library(leaps)
library(glmnet)
library("VIM")
library(corrplot)
library(caret)
install.packages("caret")
#Install required packages
library(ggplot2)
library(leaps)
library(glmnet)
library("VIM")
library(corrplot)
library(caret)
model_elnet = train(
Apps ~ ., data = subsetCollegedata2,
method = "glmnet",
trControl = trainControl(method = "cv", number = 10)
)
model_elnet = train(
Apps ~ ., data = subsetCollegedata2,
method = "glmnet",
trControl = trainControl(method = "cv", number = 10)
)
model_elnet$results
model_elnet = train(
Apps ~ ., data = subsetCollegedata2,
method = "glmnet",
trControl = trainControl(method = "cv", number = 10)
)
model_elnet$bestTune
model_elnet = train(Apps ~ ., data = subsetCollegedata2, method = "glmnet", trControl = trainControl(method = "cv", number= 10))
model_elnet$pred
model_elnet = train(Apps ~ ., data = subsetCollegedata2, method = "glmnet", trControl = trainControl(method = "cv", number= 10))
model_elnet$results
model_elnet = train(Apps ~ ., data = subsetCollegedata2, method = "glmnet", trControl = trainControl(method = "cv", number= 10))
model_elnet$metric
model_elnet = train(Apps ~ ., data = subsetCollegedata2, method = "glmnet", trControl = trainControl(method = "cv", number= 10))
min(model_elnet$results$RMSE)
model_elnet = train(Apps ~ ., data = subsetCollegedata2, method = "glmnet", trControl = trainControl(method = "cv", number= 10))
model_elnet$results$results
model_elnet = train(Apps ~ ., data = subsetCollegedata2, method = "glmnet", trControl = trainControl(method = "cv", number= 10))
model_elnet$results
model_elnet = train(Apps ~ ., data = subsetCollegedata2, method = "glmnet", trControl = trainControl(method = "cv", number= 10))
model_elnet$bestTune
model_elnet = train(Apps ~ ., data = subsetCollegedata2, method = "glmnet", trControl = trainControl(method = "cv", number= 10))
coef(elnet$finalModel, elnet$bestTune$lambda)
model_elnet = train(Apps ~ ., data = subsetCollegedata2, method = "glmnet", trControl = trainControl(method = "cv", number= 10))
coef(model_elnet$finalModel, model_elnet$bestTune$lambda)
#Create table to compare all model coefficients
Mat <- cbind(FullModel = coefficients(glmnet(XD[ , -1], subsetCollegedata2$Apps, alpha =0 ,lambda = 0 )), Shrinkage = coefficients(ridge.final), Lasso = coefficients(lasso.final), ElasticNet = coef(model_elnet$finalModel, model_elnet$bestTune$lambda))
Mat <-as.matrix(Mat)
colnames(Mat) <-c("Full Model", "Shrinkage", "Lasso", "Elastic Net")
Mat <- data.frame(Mat)
knitr::kable(Mat, caption = "Table 3")
model_elnet$bestTune
model_elnet$finalModel
model_elnet$results
set.seed(1)
#Create table to compare all model coefficients
Mat <- cbind(FullModel = coefficients(glmnet(XD[ , -1], subsetCollegedata2$Apps, alpha =0 ,lambda = 0 )), Shrinkage = coefficients(ridge.final), Lasso = coefficients(lasso.final), ElasticNet = coef(model_elnet$finalModel, model_elnet$bestTune$lambda))
Mat <-as.matrix(Mat)
colnames(Mat) <-c("Full Model", "Shrinkage", "Lasso", "Elastic Net")
Mat <- data.frame(Mat)
knitr::kable(Mat, caption = "Table 3")
set.seed(1)
#Create table to compare all model coefficients
Mat <- cbind(FullModel = coefficients(glmnet(XD[ , -1], subsetCollegedata2$Apps, alpha =0 ,lambda = 0 )), Shrinkage = coefficients(ridge.final), Lasso = coefficients(lasso.final), ElasticNet = coef(model_elnet$finalModel, model_elnet$bestTune$lambda))
Mat <-as.matrix(Mat)
colnames(Mat) <-c("Full Model", "Shrinkage", "Lasso", "Elastic Net")
Mat <- data.frame(Mat)
knitr::kable(Mat, caption = "Table 3")
set.seed(1)
#Create table to compare all model coefficients
Mat <- cbind(FullModel = coefficients(glmnet(XD[ , -1], subsetCollegedata2$Apps, alpha =0 ,lambda = 0 )), Shrinkage = coefficients(ridge.final), Lasso = coefficients(lasso.final), ElasticNet = coef(model_elnet$finalModel, model_elnet$bestTune$lambda))
Mat <-as.matrix(Mat)
colnames(Mat) <-c("Full Model", "Shrinkage", "Lasso", "Elastic Net")
Mat <- data.frame(Mat)
knitr::kable(Mat, caption = "Table 3")
set.seed(1)
#Create table to compare all model coefficients
Mat <- cbind(FullModel = coefficients(glmnet(XD[ , -1], subsetCollegedata2$Apps, alpha =0 ,lambda = 0 )), Shrinkage = coefficients(ridge.final), Lasso = coefficients(lasso.final), ElasticNet = coef(model_elnet$finalModel, model_elnet$bestTune$lambda))
Mat <-as.matrix(Mat)
colnames(Mat) <-c("Full Model", "Shrinkage", "Lasso", "Elastic Net")
Mat <- data.frame(Mat)
knitr::kable(Mat, caption = "Table 3")
#Print results to find best parameter values
model_elnet$results
knitr::opts_chunk$set(echo = FALSE)
# Import data and get general information
fooddata <- read.csv("/Users/rtf/Google Drive (fiscrt17@wfu.edu)/Fourth Year/STA363/Project3/McDonaldsProj3.csv")
ncol(fooddata)
nrow(fooddata)
#Remove daily value columns because they encode the same information as the macronutrient
subsetFooddata = fooddata[c(-5, -7, -9, -12, -14, -16, -18)]
#Remove calories from fat
View(subsetFooddata)
# Import data and get general information
fooddata <- read.csv("/Users/rtf/Google Drive (fiscrt17@wfu.edu)/Fourth Year/STA363/Project3/McDonaldsProj3.csv")
ncol(fooddata)
nrow(fooddata)
#Remove daily value columns because they encode the same information as the macronutrient
#Remove calories from fat
subsetFooddata = fooddata[c(-5, -7, -9, -12, -14, -16, -18)]
aggr_plot <- aggr(subsetFooddata, col = c("navyblue", 'red'), numbers=TRUE, sortVars=TRUE, labels=names(subsetFooddata), cex.axis=0.7, gap=3, ylab=c("Histogram of missing data", "Pattern"))
knitr::opts_chunk$set(echo = FALSE)
#Install required packages
library(ggplot2)
library(leaps)
library(glmnet)
library("VIM")
library(corrplot)
library(caret)
library(rpart)
# Import data and get general information
fooddata <- read.csv("/Users/rtf/Google Drive (fiscrt17@wfu.edu)/Fourth Year/STA363/Project3/McDonaldsProj3.csv")
ncol(fooddata)
nrow(fooddata)
#Remove daily value columns because they encode the same information as the macronutrient
#Remove calories from fat
subsetFooddata = fooddata[c(-5, -7, -9, -12, -14, -16, -18)]
aggr_plot <- aggr(subsetFooddata, col = c("navyblue", 'red'), numbers=TRUE, sortVars=TRUE, labels=names(subsetFooddata), cex.axis=0.7, gap=3, ylab=c("Histogram of missing data", "Pattern"))
aggr_plot <- aggr(subsetFooddata, col = c("navyblue", 'red'), numbers=TRUE, sortVars=TRUE, labels=names(subsetFooddata), cex.axis=0.7, gap=3, ylab=c("Histogram of missing data", "Pattern"))
#Histogram to check for missing data
aggr_plot <- aggr(subsetFooddata, col = c("navyblue", 'red'), numbers=TRUE, sortVars=TRUE, labels=names(subsetFooddata), cex.axis=0.7, gap=3, ylab=c("Histogram of missing data", "Pattern"))
#Get information on rows and columns of cleaned dataset
ncol(subsetFooddata)
nrow(subsetFooddata)
